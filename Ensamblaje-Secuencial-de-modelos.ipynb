{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e983b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn.base import BaseEstimator, RegressorMixin, clone\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5aadc5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "house = pd.read_csv('house_prices.csv')\n",
    "parkinson = pd.read_csv('parkinsons.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffc19765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columnas categóricas en house: ['Condition2', 'LandContour', 'HouseStyle', 'GarageType', 'FireplaceQu', 'Alley', 'MSZoning', 'CentralAir', 'GarageCond', 'Exterior2nd', 'BsmtFinType2', 'GarageQual', 'ExterQual', 'PavedDrive', 'LotShape', 'KitchenQual', 'SaleType', 'BsmtExposure', 'ExterCond', 'BsmtQual', 'MiscFeature', 'PoolQC']\n"
     ]
    }
   ],
   "source": [
    "# Guardamos los atributos categoricos para pasarlos a numéricos\n",
    "cat_cols = house.select_dtypes(include=['object']).columns.tolist()\n",
    "print(\"Columnas categóricas en house:\", cat_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab7541c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codificamos las variables categóricas, ponemos usamos drop_first=True para eliminar la primera categoría de cada variable y evitar la multicolinealidad\n",
    "house_enc = pd.get_dummies(house, columns=cat_cols, drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae28ffe",
   "metadata": {},
   "source": [
    "Separar features y target en house"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2ea5d6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiones X_house: (560, 109)\n",
      "Dimensiones y_house: (560,)\n"
     ]
    }
   ],
   "source": [
    "# Separamos las variables y la variable objetivo\n",
    "X_house = house_enc.drop('SalePrice', axis=1)\n",
    "y_house = house_enc['SalePrice']\n",
    "print(\"Dimensiones X_house:\", X_house.shape)\n",
    "print(\"Dimensiones y_house:\", y_house.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ce5436",
   "metadata": {},
   "source": [
    "Para Parkinsons no hay categóricas, solo separamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "210040fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiones X_parkinson: (2000, 19)\n",
      "Dimensiones y_parkinson: (2000,)\n"
     ]
    }
   ],
   "source": [
    "X_parkinson = parkinson.drop('total_UPDRS', axis=1)\n",
    "y_parkinson = parkinson['total_UPDRS']\n",
    "print(\"Dimensiones X_parkinson:\", X_parkinson.shape)\n",
    "print(\"Dimensiones y_parkinson:\", y_parkinson.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d423696a",
   "metadata": {},
   "source": [
    "Dividimos en datos de entrenamiento y prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ceda06a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "House split: (448, 109) (112, 109) (448,) (112,)\n"
     ]
    }
   ],
   "source": [
    "Xh_train, Xh_test, yh_train, yh_test = train_test_split(\n",
    "    X_house, y_house, test_size=0.2, random_state=42\n",
    ")\n",
    "print(\"House split:\", Xh_train.shape, Xh_test.shape, yh_train.shape, yh_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e29d8cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parkinsons split: (1600, 19) (400, 19) (1600,) (400,)\n"
     ]
    }
   ],
   "source": [
    "Xp_train, Xp_test, yp_train, yp_test = train_test_split(\n",
    "    X_parkinson, y_parkinson, test_size=0.2, random_state=42\n",
    ")\n",
    "print(\"Parkinsons split:\", Xp_train.shape, Xp_test.shape, yp_train.shape, yp_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "0ee894af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequentialEnsembleRegressor(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self,\n",
    "                 # Estimador a usar\n",
    "                 estimator=None,\n",
    "                 # Número de modelos a entrenar\n",
    "                 n_estimators=50,\n",
    "                 # Proporción del tamaño de la datos para cada modelo\n",
    "                 sample_size=0.8,\n",
    "                 # Tasa de aprendizaje\n",
    "                 lr=0.1,\n",
    "                 # Semilla para la aleatoriedad\n",
    "                 random_state=None,\n",
    "                 # Parámetros del estimador como max_depth, min_samples_split, etc.\n",
    "                 max_depth=None):\n",
    "        self.estimator = estimator\n",
    "        self.n_estimators = n_estimators\n",
    "        self.sample_size = sample_size\n",
    "        self.lr = lr\n",
    "        self.random_state = random_state\n",
    "        self.max_depth = max_depth\n",
    "\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        #\n",
    "        self.init_prediction_ = np.mean(y)\n",
    "        # Lista para almacenar los modelos entrenados\n",
    "        self.models = []\n",
    "        # Guardo el número de muestras\n",
    "        n_samples = X.shape[0]\n",
    "        # Inicializamos las predicciones a cero\n",
    "        pred = np.full(shape=n_samples, fill_value=self.init_prediction_)\n",
    "        # Inicializamos el generador aleatorio\n",
    "        rng = check_random_state(self.random_state)\n",
    "        \n",
    "        for i in range(self.n_estimators):\n",
    "            #Calculo el residuo actual\n",
    "            resid = y - pred\n",
    "            \n",
    "            # Creo el submodelo con los datos muestreados y los residuos como objetivos\n",
    "            model = clone(self.estimator)\n",
    "            if hasattr(model, \"random_state\"):\n",
    "                model.set_params(random_state=self.random_state)\n",
    "            # Si hay estimador_params, los aplico al modelo\n",
    "            if self.max_depth is not None:\n",
    "                if hasattr(model, \"max_depth\"):\n",
    "                    model.set_params(max_depth=self.max_depth)\n",
    "\n",
    "            # Tomo una muesta aleatoria de los datos\n",
    "            k = int(self.sample_size * n_samples)\n",
    "            # Como no hay reemplazo, replace=False\n",
    "            idx = rng.choice(n_samples, k, replace=False)\n",
    "            X_sub = X.iloc[idx]\n",
    "            y_sub = resid.iloc[idx] if hasattr(resid, \"iloc\") else resid[idx]\n",
    "\n",
    "            \n",
    "            model.fit(X_sub, y_sub)\n",
    "            # Almaceno el modelo entrenado\n",
    "            self.models.append(model)\n",
    "            # Actualizo las predicciones\n",
    "            pred += self.lr * model.predict(X)\n",
    "        return self\n",
    "\n",
    "#Devolvemos la prediccion final sumando uno a uno y multiplicando por la tasa de aprendizaje\n",
    "    def predict(self, X):\n",
    "        pred = np.full(shape=X.shape[0], fill_value=self.init_prediction_)\n",
    "        for m in self.models:\n",
    "            pred += self.lr * m.predict(X)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c59e09",
   "metadata": {},
   "source": [
    "Evaluar Housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae5c0205",
   "metadata": {},
   "outputs": [],
   "source": [
    "ens_h = SequentialEnsembleRegressor(\n",
    "    estimator=DecisionTreeRegressor(),\n",
    "    n_estimators=50,\n",
    "    sample_size=0.8,\n",
    "    lr=0.1,\n",
    "    random_state=42,\n",
    "    max_depth=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "37757d6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "House R²: 0.7071738896188212\n",
      "House MAE: 28496.63664458055\n"
     ]
    }
   ],
   "source": [
    "ens_h.fit(Xh_train, yh_train)\n",
    "yh_pred = ens_h.predict(Xh_test)\n",
    "print(\"House R²:\", r2_score(yh_test, yh_pred))\n",
    "print(\"House MAE:\", mean_absolute_error(yh_test, yh_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c243067",
   "metadata": {},
   "source": [
    "Evaluar parkingson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f629cf8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ens_p = SequentialEnsembleRegressor(\n",
    "    estimator=DecisionTreeRegressor(),\n",
    "    n_estimators=50,\n",
    "    sample_size=0.8,\n",
    "    lr=0.1,\n",
    "    random_state=42,\n",
    "    max_depth=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8d6b85bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parkinsons R²: 0.6690157606392105\n",
      "Parkinsons MAE: 4.847183397911717\n"
     ]
    }
   ],
   "source": [
    "ens_p.fit(Xp_train, yp_train)\n",
    "yp_pred = ens_p.predict(Xp_test)\n",
    "print(\"Parkinsons R²:\", r2_score(yp_test, yp_pred))\n",
    "print(\"Parkinsons MAE:\", mean_absolute_error(yp_test, yp_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9266e95",
   "metadata": {},
   "source": [
    "Validación cruzada y busqueda de hiperparametros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a150560d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "House CV R²: [0.72356832 0.75864956 0.76608286 0.78143207 0.77203406] → Media: 0.7603533739391801\n"
     ]
    }
   ],
   "source": [
    "# Se dividen los datos en 5 particiones, barajamos los datos y usamos la semilla 42 para reproducibilidad\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "scores = cross_val_score(\n",
    "    SequentialEnsembleRegressor(\n",
    "        # Estimador a usar\n",
    "        estimator=DecisionTreeRegressor(max_depth=3),\n",
    "        n_estimators=50, sample_size=0.8, lr=0.1, random_state=42\n",
    "    ),\n",
    "    Xh_train, yh_train,\n",
    "    scoring='r2', cv=cv, n_jobs=-1\n",
    "    # Se entrena el modelo con 4 de las 5 particiones y se evalua la restante usando R² como métrica\n",
    "    # Repite el proceso 5 veces, cada vez con una partición diferente como test\n",
    "    # Pongo n_jobs -1 para usar todos los núcleos disponibles en la CPU\n",
    ")\n",
    "print(\"House CV R²:\", scores, \"→ Media:\", scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "56798fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modelo base con hiperparámetros iniciales\n",
    "base_model = SequentialEnsembleRegressor(\n",
    "    estimator=DecisionTreeRegressor(max_depth=3),\n",
    "    n_estimators=50,\n",
    "    sample_size=0.8,\n",
    "    lr=0.1,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7be4e4f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "House CV R² scores: [0.72356832 0.75864956 0.76608286 0.78143207 0.77203406]\n",
      "Media R²: 0.7603533739391801\n"
     ]
    }
   ],
   "source": [
    "scores_house = cross_val_score(\n",
    "    base_model, Xh_train, yh_train,\n",
    "    scoring='r2', cv=cv, n_jobs=-1\n",
    ")\n",
    "print(\"House CV R² scores:\", scores_house)\n",
    "print(\"Media R²:\", scores_house.mean())\n",
    "# Si los modelos se parecen entre si el modelo es estable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15964a09",
   "metadata": {},
   "source": [
    "Parkinson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0f2816d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parkinsons CV R² scores: [0.63276246 0.66040138 0.63794864 0.65623765 0.6812172 ]\n",
      "Media R²: 0.6537134652991631\n"
     ]
    }
   ],
   "source": [
    "scores_parkinson = cross_val_score(\n",
    "    base_model, Xp_train, yp_train,\n",
    "    scoring='r2', cv=cv, n_jobs=-1\n",
    ")\n",
    "print(\"Parkinsons CV R² scores:\", scores_parkinson)\n",
    "print(\"Media R²:\", scores_parkinson.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b034a39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "76742f22",
   "metadata": {},
   "source": [
    "Busqueda manual de hiperparámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4da2a1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_seq_ensemble(X, y, param_grid, cv):\n",
    "    results = []\n",
    "    for n in param_grid['n_estimators']:\n",
    "        for lr in param_grid['lr']:\n",
    "            for s in param_grid['sample_size']:\n",
    "                for md in param_grid['max_depth']:\n",
    "                    model = SequentialEnsembleRegressor(\n",
    "                        estimator=DecisionTreeRegressor(),\n",
    "                        n_estimators=n,\n",
    "                        lr=lr,\n",
    "                        sample_size=s,\n",
    "                        random_state=42,\n",
    "                        max_depth=md\n",
    "                    )\n",
    "                    scores = cross_val_score(\n",
    "                        model, X, y,\n",
    "                        scoring='r2', cv=cv, n_jobs=-1\n",
    "                    )\n",
    "                    results.append({\n",
    "                        'n_estimators': n,\n",
    "                        'lr': lr,\n",
    "                        'sample_size': s,\n",
    "                        'max_depth': md,\n",
    "                        'r2_mean': scores.mean()\n",
    "                    })\n",
    "\n",
    "    df_results = pd.DataFrame(results)\n",
    "    df_results = df_results.sort_values('r2_mean', ascending=False).reset_index(drop=True)\n",
    "    print(\"Top 10 combos:\\n\", df_results.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "99f53928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 combos:\n",
      "    n_estimators   lr  sample_size  max_depth   r2_mean\n",
      "0           100  0.2          0.8          3  0.790717\n",
      "1           100  0.1          0.6          3  0.790011\n",
      "2           100  0.2          1.0          3  0.788360\n",
      "3           100  0.1          1.0          3  0.786865\n",
      "4           100  0.1          1.0          5  0.784163\n",
      "5            50  0.2          0.8          3  0.783979\n",
      "6            50  0.2          1.0          3  0.783529\n",
      "7           100  0.2          1.0          5  0.782545\n",
      "8            50  0.2          1.0          5  0.780902\n",
      "9           100  0.1          0.6          7  0.778543\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [10, 50, 100],\n",
    "    'lr': [0.01, 0.1, 0.2],\n",
    "    'sample_size': [0.6, 0.8, 1.0],\n",
    "    'max_depth': [3, 5, 7]\n",
    "}\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "grid_search_seq_ensemble(Xh_train, yh_train, param_grid, cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4984b2e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 combos:\n",
      "    n_estimators   lr  sample_size  max_depth   r2_mean\n",
      "0           150  0.1          0.6          3  0.798998\n",
      "1           150  0.1          1.0          3  0.792579\n",
      "2           150  0.2          0.8          3  0.791567\n",
      "3           100  0.2          0.8          3  0.790717\n",
      "4           100  0.1          0.6          3  0.790011\n",
      "5           150  0.2          1.0          3  0.789513\n",
      "6           100  0.2          1.0          3  0.788360\n",
      "7           100  0.1          1.0          3  0.786865\n",
      "8           150  0.1          1.0          5  0.785773\n",
      "9           100  0.1          1.0          5  0.784163\n"
     ]
    }
   ],
   "source": [
    "# Cuanto mayor sea el número de los estimadores mejor, pero también más tiempo de entrenamiento\n",
    "param_grid = {\n",
    "    'n_estimators': [ 50, 100, 150],\n",
    "    'lr': [0.01, 0.1, 0.2],\n",
    "    'sample_size': [0.6, 0.8, 1.0],\n",
    "    'max_depth': [3, 5, 7]\n",
    "}\n",
    "grid_search_seq_ensemble(Xh_train, yh_train, param_grid, cv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "13cc39c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 combos:\n",
      "    n_estimators   lr  sample_size  max_depth   r2_mean\n",
      "0           150  0.1          0.6          3  0.800152\n",
      "1           150  0.1          0.8          3  0.799826\n",
      "2           100  0.1          0.6          3  0.795902\n",
      "3           100  0.1          0.8          3  0.795000\n",
      "4            50  0.1          0.6          7  0.794500\n",
      "5           100  0.1          0.6          7  0.793904\n",
      "6           150  0.1          1.0          3  0.793392\n",
      "7           150  0.1          0.6          7  0.792711\n",
      "8           100  0.3          1.0          3  0.789663\n",
      "9           100  0.1          1.0          3  0.787341\n"
     ]
    }
   ],
   "source": [
    "# Cuanto mayor la tasa de aprendizaje, más rápido aprende el modelo, pero también puede sobreajustar más rápido\n",
    "param_grid = {\n",
    "    'n_estimators': [ 50, 100, 150],\n",
    "    'lr': [0.1, 0.2, 0.3],\n",
    "    'sample_size': [0.6, 0.8, 1.0],\n",
    "    'max_depth': [3, 5, 7]\n",
    "}\n",
    "grid_search_seq_ensemble(Xh_train, yh_train, param_grid, cv)\n",
    "# Vamos a probar con una tasa de aprendizaje mas alta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c68f61b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 combos:\n",
      "    n_estimators   lr  sample_size  max_depth   r2_mean\n",
      "0           150  0.3          1.0          3  0.809838\n",
      "1           100  0.3          1.0          3  0.806977\n",
      "2            50  0.3          1.0          3  0.802014\n",
      "3           150  0.1          0.6          3  0.798998\n",
      "4           150  0.1          1.0          3  0.792579\n",
      "5           150  0.2          0.8          3  0.791567\n",
      "6           100  0.2          0.8          3  0.790717\n",
      "7           100  0.1          0.6          3  0.790011\n",
      "8           150  0.2          1.0          3  0.789513\n",
      "9           100  0.2          1.0          3  0.788360\n"
     ]
    }
   ],
   "source": [
    "# Con tasa de aprendizaje mayor a 0.3 el modelo empieza a sobreajustar, por lo que es mejor usar una tasa de aprendizaje menor\n",
    "param_grid = {\n",
    "    'n_estimators': [ 50, 100, 150],\n",
    "    'lr': [0.1, 0.2, 0.3, 0.4],\n",
    "    'sample_size': [0.6, 0.8, 1.0],\n",
    "    'max_depth': [3, 5, 7]\n",
    "}\n",
    "grid_search_seq_ensemble(Xh_train, yh_train, param_grid, cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9bb798d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 combos:\n",
      "    n_estimators   lr  sample_size  max_depth   r2_mean\n",
      "0           150  0.1          0.6          3  0.798998\n",
      "1           150  0.2          0.8          3  0.791567\n",
      "2           100  0.2          0.8          3  0.790717\n",
      "3           100  0.1          0.6          3  0.790011\n",
      "4            50  0.2          0.8          3  0.783979\n",
      "5           150  0.1          0.6          7  0.778547\n",
      "6           100  0.1          0.6          7  0.778543\n",
      "7           150  0.1          0.6          5  0.777164\n",
      "8           100  0.1          0.6          5  0.776969\n",
      "9           150  0.1          0.8          3  0.775795\n"
     ]
    }
   ],
   "source": [
    "# Con un tamaño de muestra menor, el modelo puede aprender más rápido, pero también puede sobreajustar más rápido\n",
    "param_grid = {\n",
    "    'n_estimators': [ 50, 100, 150],\n",
    "    'lr': [0.1, 0.2, 0.3, 0.4],\n",
    "    'sample_size': [0.2, 0.6, 0.8],\n",
    "    'max_depth': [3, 5, 7]\n",
    "}\n",
    "grid_search_seq_ensemble(Xh_train, yh_train, param_grid, cv)\n",
    "# Como podemos ver, el tamaño de muestra no afecta mucho al rendimiento del modelo, pero si se reduce demasiado, el modelo puede no aprender lo suficiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cf31b775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 combos:\n",
      "    n_estimators   lr  sample_size  max_depth   r2_mean\n",
      "0           150  0.2          0.8          2  0.801028\n",
      "1           100  0.2          0.8          2  0.793301\n",
      "2            50  0.2          0.8          2  0.791917\n",
      "3           150  0.1          0.8          2  0.789721\n",
      "4           150  0.1          1.0          2  0.786229\n",
      "5           150  0.1          0.6          2  0.785940\n",
      "6           150  0.1          1.0          5  0.785773\n",
      "7           100  0.1          1.0          5  0.784163\n",
      "8           150  0.2          1.0          5  0.783008\n",
      "9           100  0.2          1.0          5  0.782545\n"
     ]
    }
   ],
   "source": [
    "# Con max_depth mayor, el modelo puede aprender más rápido, pero también puede sobreajustar más rápido\n",
    "param_grid = {\n",
    "    'n_estimators': [ 50, 100, 150],\n",
    "    'lr': [0.1, 0.2, 0.3, 0.4],\n",
    "    'sample_size': [0.6, 0.8, 1.0],\n",
    "    'max_depth': [2, 5, 7]\n",
    "}\n",
    "grid_search_seq_ensemble(Xh_train, yh_train, param_grid, cv)\n",
    "# Como podemos ver, poniendo un max_depth menor el modelo reduce el sobreajuste, mientras que con un max_depth mayor puede captar patrones más complejos\n",
    "# pero con mayor riesgo de memorizar ruido."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818b27da",
   "metadata": {},
   "source": [
    "Evaluación en test con los mejores hiperparámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3413aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "House Test R²: 0.7191232968866706\n",
      "House Test MAE: 29074.149712968017\n"
     ]
    }
   ],
   "source": [
    "# Top 10 combos:\n",
    "#     n_estimators   lr  sample_size  max_depth   r2_mean\n",
    "# 0           150  0.3          1.0          3  0.809838\n",
    "\n",
    "# Como nuestro dataset de viviendas sólo tiene 560 registros, la partición 80 %–20 % ya deja 448 ejemplos para entrenar. \n",
    "# En 5‐fold CV, cada entrenamiento ve sólo 358 filas, lo que resulta insuficiente para estabilizar un ensamble de árboles profundos. \n",
    "# Por eso, cuando comparo configuraciones en validación cruzada (por ejemplo, (150, 0.3, 1.0, 3) con r2_mean ≈ 0.81), al evaluar en el test final (112 filas)\n",
    "#   puede caer a ~0.70 según la semilla y la combinación de hiperparámetros. \n",
    "# Esa variación es esperable: con tan pocos datos, pequeñas diferencias en el muestreo afectan mucho el ajuste. \n",
    "# Si tuviéramos 5 000 filas, un 20 % de test nos dejaría 4 000 para entrenar y cada pliegue tendría 3 200, con lo que la métrica sería más estable. \n",
    "# Pero con 560 filas, incluso cambiar max_depth de 3 a 5 o bajar la tasa de aprendizaje altera fuertemente el patrón de residuos que corrige cada árbol. \n",
    "# Por eso, al presentar los resultados debo advertir que la variabilidad se debe principalmente al tamaño reducido del dataset y al alto poder de los árboles frente a pocos ejemplos.\n",
    "\n",
    "\n",
    "best_house = SequentialEnsembleRegressor(\n",
    "    estimator=DecisionTreeRegressor(),\n",
    "    n_estimators=150,\n",
    "    lr=0.3,\n",
    "    sample_size=0.8,\n",
    "    random_state=42,\n",
    "    max_depth=3\n",
    ")\n",
    "best_house.fit(Xh_train, yh_train)\n",
    "yh_test_pred = best_house.predict(Xh_test)\n",
    "print(\"House Test R²:\", r2_score(yh_test, yh_test_pred))\n",
    "print(\"House Test MAE:\", mean_absolute_error(yh_test, yh_test_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a217c4",
   "metadata": {},
   "source": [
    "Busqueda de hiperparametros para parkinson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "05067f52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 combos:\n",
      "    n_estimators   lr  sample_size  max_depth   r2_mean\n",
      "0           100  0.1          1.0          7  0.930023\n",
      "1           100  0.2          1.0          7  0.925988\n",
      "2            50  0.2          1.0          7  0.924238\n",
      "3           100  0.1          0.8          7  0.922532\n",
      "4            50  0.1          1.0          7  0.920539\n",
      "5           100  0.2          0.8          7  0.918401\n",
      "6            50  0.2          0.8          7  0.916307\n",
      "7            50  0.1          0.8          7  0.914560\n",
      "8           100  0.1          0.6          7  0.904985\n",
      "9           100  0.2          1.0          5  0.899191\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [ 10, 50, 100],\n",
    "    'lr': [0.01, 0.1, 0.2],\n",
    "    'sample_size': [0.6, 0.8, 1.0],\n",
    "    'max_depth': [3, 5, 7]\n",
    "}\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "grid_search_seq_ensemble(Xp_train, yp_train, param_grid, cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "df5bc77e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parkinsons Test R²: 0.936545781009529\n",
      "Parkinsons Test MAE: 1.615012580924182\n"
     ]
    }
   ],
   "source": [
    "# Top 10 combos:\n",
    "#     n_estimators   lr  sample_size  max_depth   r2_mean\n",
    "# 0           100  0.1          1.0          7  0.930023\n",
    "best_park = SequentialEnsembleRegressor(\n",
    "    estimator=DecisionTreeRegressor(),\n",
    "    n_estimators=100,\n",
    "    lr=0.1,\n",
    "    sample_size=1,\n",
    "    random_state=42,\n",
    "    max_depth=7\n",
    ")\n",
    "best_park.fit(Xp_train, yp_train)\n",
    "yp_test_pred = best_park.predict(Xp_test)\n",
    "print(\"Parkinsons Test R²:\", r2_score(yp_test, yp_test_pred))\n",
    "print(\"Parkinsons Test MAE:\", mean_absolute_error(yp_test, yp_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e864adc",
   "metadata": {},
   "source": [
    "Ahora con EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e8aafb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequentialEnsembleRegressorEarlyStopping(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, estimator=None, n_estimators=50, sample_size=0.8, lr=0.1, random_state=None, patience=10, max_depth=None):\n",
    "        self.estimator = estimator\n",
    "        self.n_estimators = n_estimators\n",
    "        self.sample_size = sample_size\n",
    "        self.lr = lr\n",
    "        self.random_state = random_state\n",
    "        self.patience = patience\n",
    "        self.max_depth = max_depth\n",
    "\n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        # Inicializar lista de modelos\n",
    "        self.models = []\n",
    "        # Guardar predicción inicial como media de y\n",
    "        self.init_prediction_ = np.mean(y)\n",
    "        n_samples = X.shape[0]\n",
    "        # Predicciones actuales (comenzamos con ini_prediction_)\n",
    "        pred = np.full(shape=n_samples, fill_value=self.init_prediction_, dtype=float)\n",
    "        # Generador aleatorio\n",
    "        rng = check_random_state(self.random_state)\n",
    "\n",
    "        best_score = -np.inf\n",
    "        best_iter = 0\n",
    "        patience_counter = 0\n",
    "\n",
    "        # Iterar sobre el número máximo de estimadores\n",
    "        for m in range(self.n_estimators):\n",
    "            # Calcular residuos\n",
    "            resid = y - pred\n",
    "\n",
    "            # Muestreo sin reemplazo\n",
    "            k = int(self.sample_size * n_samples)\n",
    "            idx = rng.choice(n_samples, k, replace=False)\n",
    "            X_sub = X.iloc[idx] if hasattr(X, \"iloc\") else X[idx]\n",
    "            y_sub = resid[idx] if not hasattr(resid, \"iloc\") else resid.iloc[idx]\n",
    "\n",
    "            # Crear y configurar modelo\n",
    "            model = clone(self.estimator)\n",
    "            if hasattr(model, \"random_state\"):\n",
    "                model.set_params(random_state=self.random_state)\n",
    "            if self.max_depth is not None and hasattr(model, \"max_depth\"):\n",
    "                model.set_params(max_depth=self.max_depth)\n",
    "\n",
    "            # Entrenar\n",
    "            model.fit(X_sub, y_sub)\n",
    "            self.models.append(model)\n",
    "\n",
    "            # Actualizar predicciones sobre set de entrenamiento\n",
    "            pred += self.lr * model.predict(X)\n",
    "\n",
    "            # Early stopping si se proporciona validación\n",
    "            if X_val is not None and y_val is not None:\n",
    "                # Calcular predicción en validación\n",
    "                val_pred = np.full(shape=y_val.shape[0], fill_value=self.init_prediction_, dtype=float)\n",
    "                for mm in self.models:\n",
    "                    val_pred += self.lr * mm.predict(X_val)\n",
    "                score = r2_score(y_val, val_pred)\n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    best_iter = m\n",
    "                    patience_counter = 0\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                    if patience_counter >= self.patience:\n",
    "                        # Cortar modelos hasta la mejor iteración\n",
    "                        self.models = self.models[:best_iter+1]\n",
    "                        break\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Predecir sumando la predicción inicial y contribución de cada modelo\n",
    "        pred = np.full(shape=X.shape[0], fill_value=self.init_prediction_, dtype=float)\n",
    "        for m in self.models:\n",
    "            pred += self.lr * m.predict(X)\n",
    "        return pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9e1de6fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R² con early stopping: 0.9936241094921748\n",
      "MAE con early stopping: 5443.094420934293\n"
     ]
    }
   ],
   "source": [
    "es = SequentialEnsembleRegressorEarlyStopping(\n",
    "    estimator=DecisionTreeRegressor(),\n",
    "    n_estimators=150,\n",
    "    sample_size=1,\n",
    "    lr=0.3,\n",
    "    random_state=42,\n",
    "    patience=10,\n",
    "    max_depth=3\n",
    ")\n",
    "\n",
    "es.fit(X_house, y_house, X_val=Xh_test, y_val=yh_test)\n",
    "\n",
    "yh_es = es.predict(Xh_test)\n",
    "print(\"R² con early stopping:\", r2_score(yh_test, yh_es))\n",
    "print(\"MAE con early stopping:\", mean_absolute_error(yh_test, yh_es))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc63804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R² Parkinsons con early stopping: 0.9968557350793672\n",
      "MAE Parkinsons con early stopping: 0.39197175101948145\n"
     ]
    }
   ],
   "source": [
    "es_p = SequentialEnsembleRegressorEarlyStopping(\n",
    "    estimator=DecisionTreeRegressor(),\n",
    "    n_estimators=100,\n",
    "    sample_size=1,\n",
    "    lr=0.1,\n",
    "    random_state=42,\n",
    "    patience=10,\n",
    "    max_depth=7\n",
    ")\n",
    "es_p.fit(X_parkinson, y_parkinson, X_val=Xp_test, y_val=yp_test)\n",
    "yp_es = es_p.predict(Xp_test)\n",
    "print(\"R² Parkinsons con early stopping:\", r2_score(yp_test, yp_es))\n",
    "print(\"MAE Parkinsons con early stopping:\", mean_absolute_error(yp_test, yp_es))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4983d26a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e2005fd8",
   "metadata": {},
   "source": [
    "================================"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ddc5d3e",
   "metadata": {},
   "source": [
    "Validación cruzada inicial con LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14cb5a39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "House CV R² (LR): [0.71189146 0.63451049 0.77140362 0.7845024  0.71603573 0.73816302\n",
      " 0.78719992 0.8385209  0.42194304 0.64213359] → media: 0.7046304172332045\n",
      "Parkinsons CV R² (LR): [0.14906191 0.24233909 0.23572654 0.21875165 0.15566507 0.07107035\n",
      " 0.12334617 0.12858158 0.18330935 0.09996638] → media: 0.16078180902492384\n"
     ]
    }
   ],
   "source": [
    "base_lr = SequentialEnsembleRegressor(\n",
    "    estimator=LinearRegression(fit_intercept=True),\n",
    "    n_estimators=50,\n",
    "    sample_size=0.8,\n",
    "    lr=0.1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "scores_h_lr = cross_val_score(base_lr, Xh_train, yh_train, \n",
    "                              scoring='r2', cv=cv, n_jobs=-1)\n",
    "scores_p_lr = cross_val_score(base_lr, Xp_train, yp_train, \n",
    "                              scoring='r2', cv=cv, n_jobs=-1)\n",
    "\n",
    "print(\"House CV R² (LR):\", scores_h_lr, \"→ media:\", scores_h_lr.mean())\n",
    "print(\"Parkinsons CV R² (LR):\", scores_p_lr, \"→ media:\", scores_p_lr.mean())\n",
    "# Para house funciona un poco mejor el linear regression pues no contempla datos muy complejos, simplemente numero de garajes, metros cuadrados, etc.\n",
    "# Mientras que en parkinson funciona mejor el decision tree pues hay más variabilidad en los datos y es más complejo de modelar con una regresión lineal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b13ad15",
   "metadata": {},
   "source": [
    "Busqueda manual de mejores hiperparámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2ec8cc39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 combos House (LR):\n",
      "     n_estimators   lr  sample_size  fit_intercept   r2_mean\n",
      "16            10  0.2          1.0           True  0.724462\n",
      "17            10  0.2          1.0          False  0.709113\n",
      "28            50  0.1          1.0           True  0.706787\n",
      "32            50  0.2          0.8           True  0.706090\n",
      "46           100  0.1          1.0           True  0.705334\n",
      "\n",
      "Top 5 combos Parkinsons (LR):\n",
      "     n_estimators   lr  sample_size  fit_intercept   r2_mean\n",
      "14            10  0.2          0.8           True  0.163951\n",
      "16            10  0.2          1.0           True  0.163406\n",
      "12            10  0.2          0.6           True  0.162430\n",
      "28            50  0.1          1.0           True  0.162171\n",
      "46           100  0.1          1.0           True  0.161994\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [10, 50, 100],\n",
    "    'lr': [0.01, 0.1, 0.2],\n",
    "    'sample_size': [0.6, 0.8, 1.0],\n",
    "    'fit_intercept': [True, False]\n",
    "}\n",
    "\n",
    "def grid_search(X, y):\n",
    "    results = []\n",
    "    for n in param_grid['n_estimators']:\n",
    "        for lr in param_grid['lr']:\n",
    "            for s in param_grid['sample_size']:\n",
    "                for fi in param_grid['fit_intercept']:\n",
    "                    model = SequentialEnsembleRegressor(\n",
    "                        estimator=LinearRegression(fit_intercept=fi),\n",
    "                        n_estimators=n,\n",
    "                        lr=lr,\n",
    "                        sample_size=s,\n",
    "                        random_state=42\n",
    "                    )\n",
    "                    scores = cross_val_score(\n",
    "                        model, X, y, scoring='r2', cv=cv, n_jobs=-1\n",
    "                    )\n",
    "                    results.append({\n",
    "                        'n_estimators': n,\n",
    "                        'lr': lr,\n",
    "                        'sample_size': s,\n",
    "                        'fit_intercept': fi,\n",
    "                        'r2_mean': scores.mean()\n",
    "                    })\n",
    "    return pd.DataFrame(results).sort_values('r2_mean', ascending=False)\n",
    "\n",
    "df_h_lr = grid_search(Xh_train, yh_train)\n",
    "df_p_lr = grid_search(Xp_train, yp_train)\n",
    "\n",
    "print(\"Top 5 combos House (LR):\\n\", df_h_lr.head(5))\n",
    "print(\"\\nTop 5 combos Parkinsons (LR):\\n\", df_p_lr.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ece717e",
   "metadata": {},
   "source": [
    "Evaluación final del test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "282b6fd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "House Test (LR) R²: 0.766222639788844 MAE: 25257.28627191641\n"
     ]
    }
   ],
   "source": [
    "#Extraer top-1 de House\n",
    "best_h = df_h_lr.iloc[0]\n",
    "best_lr_h = SequentialEnsembleRegressor(\n",
    "    estimator=LinearRegression(fit_intercept=bool(best_h['fit_intercept'])),\n",
    "    n_estimators=int(best_h['n_estimators']),\n",
    "    lr=float(best_h['lr']),\n",
    "    sample_size=float(best_h['sample_size']),\n",
    "    random_state=42\n",
    ")\n",
    "best_lr_h.fit(Xh_train, yh_train)\n",
    "yh_lr_pred = best_lr_h.predict(Xh_test)\n",
    "print(\"House Test (LR) R²:\", r2_score(yh_test, yh_lr_pred),\n",
    "      \"MAE:\", mean_absolute_error(yh_test, yh_lr_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5f5f4612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parkinsons Test (LR) R²: 0.14887395829827899 MAE: 8.077265793598144\n"
     ]
    }
   ],
   "source": [
    "# Extraer top-1 de Parkinsons\n",
    "best_p = df_p_lr.iloc[0]\n",
    "best_lr_p = SequentialEnsembleRegressor(\n",
    "    estimator=LinearRegression(fit_intercept=bool(best_p['fit_intercept'])),\n",
    "    n_estimators=int(best_p['n_estimators']),\n",
    "    lr=float(best_p['lr']),\n",
    "    sample_size=float(best_p['sample_size']),\n",
    "    random_state=42\n",
    ")\n",
    "best_lr_p.fit(Xp_train, yp_train)\n",
    "yp_lr_pred = best_lr_p.predict(Xp_test)\n",
    "print(\"Parkinsons Test (LR) R²:\", r2_score(yp_test, yp_lr_pred),\n",
    "      \"MAE:\", mean_absolute_error(yp_test, yp_lr_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d989ba89",
   "metadata": {},
   "source": [
    "EarlyStoping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "0f3e2f04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "House (LR + early stopping) R²: 0.766222639788844\n",
      "House (LR + early stopping) MAE: 25257.28627191641\n"
     ]
    }
   ],
   "source": [
    "es_lr = SequentialEnsembleRegressorEarlyStopping(\n",
    "    estimator=LinearRegression(),\n",
    "    n_estimators=10,\n",
    "    sample_size=1.0,      \n",
    "    lr=0.2,\n",
    "    random_state=42,\n",
    "    patience=10,          \n",
    ")\n",
    "es_lr.fit(Xh_train, yh_train)\n",
    "yh_es_lr = es_lr.predict(Xh_test)\n",
    "\n",
    "print(\"House (LR + early stopping) R²:\", r2_score(yh_test, yh_es_lr))\n",
    "print(\"House (LR + early stopping) MAE:\", mean_absolute_error(yh_test, yh_es_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b4b0a26a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parkinsons (LR + early stopping) R²: 0.14887395829827899\n",
      "Parkinsons (LR + early stopping) MAE: 8.077265793598144\n"
     ]
    }
   ],
   "source": [
    "es_lr_p = SequentialEnsembleRegressorEarlyStopping(\n",
    "    estimator=LinearRegression(),\n",
    "    n_estimators=10,\n",
    "    sample_size=0.8,\n",
    "    lr=0.2,\n",
    "    random_state=42,\n",
    "    patience=10\n",
    ")\n",
    "es_lr_p.fit(Xp_train, yp_train)\n",
    "yp_es_lr = es_lr_p.predict(Xp_test)\n",
    "\n",
    "print(\"Parkinsons (LR + early stopping) R²:\", r2_score(yp_test, yp_es_lr))\n",
    "print(\"Parkinsons (LR + early stopping) MAE:\", mean_absolute_error(yp_test, yp_es_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f04375",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
